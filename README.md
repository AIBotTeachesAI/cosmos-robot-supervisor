# Cosmos Robot Supervisor
**Hierarchical Vision-Based Safety Monitoring for Physical AI**

[![NVIDIA Cosmos Cookoff 2026](https://img.shields.io/badge/NVIDIA-Cosmos%20Cookoff%202026-76B900?style=for-the-badge&logo=nvidia)](https://github.com/AIBotTeachesAI/cosmos-robot-supervisor)
[![Physical AI](https://img.shields.io/badge/Physical-AI-orange?style=for-the-badge)](#)
[![Real Hardware](https://img.shields.io/badge/Real-Hardware-blue?style=for-the-badge)](#)

> ğŸ¯ **Vision-based supervisor using NVIDIA Cosmos Reason2 to monitor real robot manipulation tasks and make safety decisions**

## Architecture Diagram

<p align="center">
  <img src="assets/architecture_diagram.png" alt="System Architecture" width="800"/>
</p>

<p align="center">
  <img src="assets/demo_preview.gif" alt="Robot Supervisor in Action" width="700"/>
</p>


---

## ğŸ¬ Demo Video

**ğŸ‘‰ [Watch 3-minute demo here](assets/demo_video.mp4)**

See the system supervise a real SO-101 robotic arm across success, obstruction, and failure scenarios.

---

## ğŸ”¥ The Problem

Robots don't fail because motion planning breaks â€” they fail because **the world changes**:

- A human hand enters the workspace â†’ **collision risk**
- An object slips from the gripper â†’ **task failure**
- An obstacle appears unexpectedly â†’ **unsafe to continue**

Traditional robots can't recognize these situations from vision alone. They need explicit rules for every edge case, or they silently proceed into failure.

**This project solves that** by adding a vision-based supervisor that watches the robot and decides: should it proceed, stop, or retry?

---

## âœ¨ What We Built

A **hierarchical reasoning system** using NVIDIA Cosmos Reason2 that monitors a real robotic arm performing block manipulation and outputs structured safety decisions.

### System Architecture

```
Robot Egocentric Video (15s)
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Level 1: Visual Classification â”‚  â† Cosmos Reason2
â”‚  Detects: SUCCESS_LIFT /        â”‚
â”‚           OBSTRUCTION /          â”‚
â”‚           MISS_GRASP             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Level 2: Decision Mapping      â”‚  â† Cosmos Reason2
â”‚  Maps to: PROCEED /              â”‚
â”‚           ABORT /                â”‚
â”‚           REPLAN                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â†“
    Supervisor Decision
```

**Key Design Principle:** Cosmos Reason2 acts as a **reasoning layer only** â€” it does not control the robot. It supervises and advises.

---

## ğŸ¯ Results

Tested on real SO-101 robotic arm with egocentric gripper camera.

| Scenario | Visual Evidence | Level 1 Classification | Level 2 Decision | Confidence |
|----------|----------------|----------------------|-----------------|------------|
| **Safe Grasp** | Block lifted and held in gripper | SUCCESS_LIFT | âœ… **PROCEED** | 99% |
| **Obstruction** | Soft toy blocks gripper path | OBSTRUCTION | ğŸ›‘ **ABORT** | 99% |
| **Grasp Failure** | Block pushed down / not lifted/ block moved | MISS_GRASP | ğŸ”„ **REPLAN** | 99% |

**All scenarios correctly classified with high confidence.**

### Example Output (Approach 1)

```json
{
  "action": "PROCEED",
  "why": "The orange block is clearly lifted off the surface and held in the gripper",
  "confidence": 0.99
}
```

---

## ğŸ’¡ Key Insight: Why Full-Context Reasoning Wins

We explored two architectural approaches:

### Approach Comparison

| Aspect | **Approach 1** âœ… (Final) | Approach 2 (Explored) |
|--------|--------------------------|----------------------|
| **Level 1 Input** | Full 15s video | 2s sliding window clips |
| **Level 1 Prompt** | Task-specific labels | Generic "describe what you see" |
| **Level 1 Output** | Structured JSON directly | Natural language â†’ heuristics needed |
| **Level 2** | Clean labelâ†’action mapping | Rule-based state extraction |
| **Result** | 99% confidence, consistent | Inconsistent, required heavy post-processing |

**Key Learning:** 
Vision-based reasoning systems achieve highest reliability when given **complete temporal context** paired with **task-specific output schemas**. Our experiments showed that 2-second sliding windows, while providing fine-grained temporal detail, fragment the manipulation sequence and make success/failure distinctions less clearâ€”particularly for small objects where context across the full action matters.


This architectural insight is the project's core contribution â€” demonstrating how input scope and prompt structure directly impact decision reliability in physical AI systems.
---

## âš™ï¸ Hardware Setup

- **Robot:** SO-101 Robotic Arm
- **Camera:** Gripper-mounted (egocentric view)
- **Environment:** Garage workspace with brown table
- **Object:** Orange Lego block
- **Inference:** GCP Deep Learning VM with L4 GPU (24GB)
- **Model:** NVIDIA Cosmos Reason2-2B

<p align="center">
  <img src="assets/so_101.jpg" alt="SO-101 Setup" width="600"/>
</p>

---

## ğŸš€  Installation & Usage


ğŸ“– **Full installation guide:** See [SETUP.md](SETUP.md)

---

## ğŸ“ Repository Structure

```
cosmos-robot-supervisor/
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ approach1/                    # Final pipeline (full-video reasoning)
â”‚   â”‚   â”œâ”€â”€ level1_classify_full_video.py
â”‚   â”‚   â”œâ”€â”€ run_level1_all.py         # Convenience wrapper
â”‚   â”‚   â””â”€â”€ level2_from_fullvideo_files.py
â”‚   â”‚
â”‚   â””â”€â”€ approach2_exploration/        # Alternative approach (sliding clips)
â”‚       â”œâ”€â”€ level1_explain_batch_state_label.py
â”‚       â”œâ”€â”€ level2_from_label_files.py
â”‚       â””â”€â”€ video_utils/
â”‚           â””â”€â”€ make_slowmo_and_sliding_clips.sh
â”‚
â”œâ”€â”€ videos/
â”‚   â”œâ”€â”€ inputs/                       # Original test videos
â”‚   â”‚   â”œâ”€â”€ success/
â”‚   â”‚   â”œâ”€â”€ obstruction/
â”‚   â”‚   â””â”€â”€ push_down/
â”‚   â”‚
â”‚   â””â”€â”€ approach2_exploration/
â”‚       â””â”€â”€ derived_inputs/           # Generated clips for Approach 2
â”‚
â”œâ”€â”€ outputs/
â”‚   â”œâ”€â”€ approach1/                    # Results from final pipeline
â”‚   â”‚   â”œâ”€â”€ level1_fullvideo_*.jsonl
â”‚   â”‚   â”œâ”€â”€ level2_fullvideo_*.json
â”‚   â”‚   â””â”€â”€ summary.json              # All scenarios
â”‚   â”‚
â”‚   â””â”€â”€ approach2_exploration/        # Results from exploratory approach
â”‚
â”œâ”€â”€ assets/
â”‚   â”œâ”€â”€ demo_video.mp4
â”‚   â”œâ”€â”€ architecture_diagram.png
â”‚   â”œâ”€â”€ demo_preview.gif
â”‚   â””â”€â”€ so_101.jpg                        # Hardware setup photo
â”‚ 
â”œâ”€â”€ README.md
â”œâ”€â”€ SETUP.md                          # Detailed installation guide
â””â”€â”€ requirements.txt
```

---

## ğŸ”¬ Technical Details

### Level 1: Visual Classification

Cosmos Reason2 analyzes the full 15-second manipulation video and outputs structured classification:

**Prompt Strategy:**
- Task-specific labels (SUCCESS_LIFT, OBSTRUCTION, MISS_GRASP)
- Forces structured JSON output
- Includes visual evidence bullets

**Why this works:** Clear label space + full temporal context = reliable classification.

### Level 2: Supervisor Decision

Cosmos Reason2 maps Level 1 classification to actionable supervisor decision:

```
SUCCESS_LIFT â†’ PROCEED (safe to continue)
OBSTRUCTION  â†’ ABORT   (unsafe, stop immediately)
MISS_GRASP   â†’ REPLAN  (retry required)
```

**Design note:** Level 2 is intentionally simple â€” this is a **decision mapping layer**, not complex reasoning. The heavy reasoning happens in Level 1.

---

## ğŸ“ What We Learned

### âœ… What Worked
- **Full-context reasoning:** 15s videos captured complete manipulation sequences more reliably than 2s clips
- **Structured prompting:** Task-specific labels provided clearer output than open-ended descriptions
- **Two-level hierarchy:** Separating classification from decision mapping improved interpretability

### ğŸ” What We Explored
- **Sliding window approach:** Enabled frame-by-frame analysis but fragmented action sequences
- **Heuristic state extraction:** Required additional processing to convert natural language observations into structured states
- **Temporal aggregation:** Aggregating temporal state proved effective when input representations were well-formed

### ğŸ’­ Key Takeaway


Vision-based reasoning for physical AI benefits significantly from **thoughtful prompt engineering and input design**. Providing complete temporal context and structured output schemasâ€”rather than fragmented clips and open-ended promptsâ€”led to more consistent and confident supervisory decisions in our experiments.
---

## ğŸš§ Future Work

- **Cosmos Predict integration:** Visualize predicted failure trajectories after REPLAN decision
- **Multi-camera fusion:** Combine gripper + external camera views for spatial reasoning
- **Multi-step tasks:** Extend to longer manipulation sequences (stack, pour, handover)
- **Real-time deployment:** Integrate with robot control loop for live supervision

---

## ğŸ“Š Impact

This system demonstrates a **practical safety layer** for physical AI:

âœ… **Safer human-robot collaboration** â€” detects workspace intrusions  
âœ… **Early failure detection** â€” catches handling errors before damage  
âœ… **Operational reliability** â€” reduces silent failures in deployment  
âœ… **Generalizable approach** â€” same architecture applies to other manipulation tasks  

The hierarchical reasoning pattern shown here can be adapted to autonomous vehicles, warehouse robots, surgical systems, and any physical AI that needs vision-based safety monitoring.

---

## ğŸ™ Acknowledgments

Built for **NVIDIA Cosmos Cookoff 2026**

**Powered by:**
- [NVIDIA Cosmos Reason2](https://github.com/nvidia-cosmos/cosmos-reason2) â€” Foundation model for physical AI reasoning
- SO-101 Robotic Arm â€” Real hardware testing platform
- GCP Deep Learning VMs â€” GPU infrastructure

**Special thanks to:**
- NVIDIA Cosmos team for the incredible reasoning model and documentation
- Competition organizers and community support via Discord
- Open-source robotics community

---

<p align="center">
  <strong>ğŸ¤– Built with real hardware Â· ğŸ§  Powered by Cosmos Reason2 Â· ğŸ† NVIDIA Cosmos Cookoff 2026</strong>
</p>
